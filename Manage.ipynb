{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9935ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import constants           #I imported all of our required modules\n",
    "import KNN as knn\n",
    "import Lasso as lasso\n",
    "import MLR as mlr\n",
    "import NNRegressor as nnreg\n",
    "import RF as rf\n",
    "import MLRMLPRegressor as mlrmlp\n",
    "import utils\n",
    "import csv\n",
    "\n",
    "# we have some functions here. every of these functions do special things,after definition of functions 2functions will\n",
    "# be called,initialize() and run_experiment().\n",
    "\n",
    "final_results = []  #this is a parameter for \n",
    "\n",
    "def write_results_to_csv(experiment_name):\n",
    "    file_path = f\"all results/{experiment_name}/results/{constants.scores_output_csv_file}\"\n",
    "    with open(file_path, 'a', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        for row in final_results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    plt.figure(figsize=(16, 6))     #dimention of printed diagrams(by experience I came to this result this is a good size)\n",
    "    font = {'family' : 'Times new roman',\n",
    "            'size'   : 12}\n",
    "    mpl.rc('font', **font)\n",
    "    if (not (Path.exists(Path(\"all results\")))): #if this folder won't exist, go and create it.\n",
    "        Path.mkdir(Path(\"all results\"), exist_ok=True) #mkdir for make directory #exist_ok, to not getting error if we had folder\n",
    "\n",
    "    for experiment in constants.experiments:   #for every experiment we defined in constants.py we get the title of experiment.\n",
    "                                                \n",
    "        experiment_name = experiment[\"title\"]\n",
    "        create_output_directories(experiment_name) #create output folders # we have results folder and pickles folder in all results\n",
    "    \n",
    "        \n",
    "        if(experiment[\"generate_cft_result\"] == True):  #for every experiment, it asks we want the results of cool farm tool or not?\n",
    "            experiment_name = experiment[\"title\"] + \" - cool farm tool\"\n",
    "            create_output_directories(experiment_name)\n",
    "        \n",
    "\n",
    "def create_output_directories(experiment_name: str):     # example.all results-->Iranian potato only-->pickles & results\n",
    "    if (not (Path.exists(Path(f\"all results/{experiment_name}\")))): #this function will bulid these folders \n",
    "        Path.mkdir(Path(f\"all results/{experiment_name}\"), exist_ok=True) #  will build (results.txt) and (scores.csv)\n",
    "            \n",
    "    if (not (Path.exists(Path(f\"all results/{experiment_name}/results\")))):\n",
    "        Path.mkdir(Path(f\"all results/{experiment_name}/results\"), exist_ok=True) \n",
    "        \n",
    "    if (not (Path.exists(Path(f\"all results/{experiment_name}/pickles\")))):\n",
    "        Path.mkdir(Path(f\"all results/{experiment_name}/pickles\"), exist_ok=True)\n",
    "\n",
    "    with open(f\"all results/{experiment_name}/results/{constants.scores_output_file_name}\", \"w\") as f:\n",
    "        f.close()\n",
    "        \n",
    "    with open(f\"all results/{experiment_name}/results/{constants.scores_output_csv_file}\", \"w\") as f:\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def read_datasets_and_prepare_model(file_name, datasets_dic, experiment_name, cft_evaluate, experiment_type):\n",
    "    train_test_datasets = []  \n",
    "    X_train = np.empty([1, 1])\n",
    "    y_train = np.empty([1, 1])\n",
    "\n",
    "    if (Path.exists(Path(f\"all results/{experiment_name}/pickles/{file_name}\"))):  #if already loaded or not\n",
    "        train_test_datasets = pickle.load(\n",
    "            open(f\"all results/{experiment_name}/pickles/{file_name}\", 'rb'))\n",
    "\n",
    "        for index in range(len(datasets_dic)):\n",
    "            if (datasets_dic[index][\"use_in_train\"]):\n",
    "                if (index == 0):\n",
    "                    X_train = train_test_datasets[index][2].values\n",
    "                    y_train = train_test_datasets[index][4].values\n",
    "                else:\n",
    "                    X_train = np.concatenate(\n",
    "                        (X_train, train_test_datasets[index][2].values))\n",
    "                    y_train = np.concatenate(\n",
    "                        (y_train, train_test_datasets[index][4].values))\n",
    "\n",
    "        y_train = np.ravel(y_train)\n",
    "        return (train_test_datasets, X_train, y_train)\n",
    "\n",
    "    for index in range(len(datasets_dic)):\n",
    "        trainCropDataset, X_train_crop, y_train_crop = utils.load_dataset(                       \n",
    "            datasets_dic[index][\"train_file_path\"], cft_evaluate, experiment_type)      #call utils for train dataset\n",
    "        testCropDataset, X_test_crop, y_test_crop = utils.load_dataset(\n",
    "            datasets_dic[index][\"test_file_path\"], cft_evaluate, experiment_type)       # call utils for test dataset\n",
    "\n",
    "        train_test_datasets.append(\n",
    "            [trainCropDataset, testCropDataset, X_train_crop, X_test_crop, y_train_crop, y_test_crop])\n",
    "\n",
    "        if (datasets_dic[index][\"use_in_train\"]):\n",
    "            if (index == 0):\n",
    "                X_train = X_train_crop.values\n",
    "                y_train = y_train_crop.values\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X_train_crop.values))\n",
    "                y_train = np.concatenate((y_train, y_train_crop.values))\n",
    "\n",
    "    pickle.dump(train_test_datasets, open(f\"all results/{experiment_name}/pickles/{file_name}\", 'wb'))\n",
    "    y_train = np.ravel(y_train)\n",
    "    return (train_test_datasets, X_train, y_train)  #if we had pickles before, we do not need to load dataset and ztrain again\n",
    "\n",
    "\n",
    "def create_test_results(model_name: str, model_cv: GridSearchCV, train_test_datasets: list, experiment: dict, experiment_name: str, optimized_features: list = []):\n",
    "    file_path = f\"all results/{experiment_name}/results/{constants.scores_output_file_name}\"\n",
    "    \n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#### {model_name} ####\\n\")\n",
    "        f.write(f\"Best params: {model_cv.best_params_}\\n\")\n",
    "        f.write(f\"Best score (r2): {model_cv.best_score_}\\n\")\n",
    "        final_results.append([model_name, f\"{model_cv.best_params_}\", model_cv.best_score_])\n",
    "\n",
    "        X_test_total = np.empty([1, 1])\n",
    "        y_test_total = np.empty([1, 1])\n",
    "\n",
    "        accumulator = False\n",
    "\n",
    "        for i in range(len(experiment[\"datasets\"])):\n",
    "            details = []\n",
    "            if (experiment[\"datasets\"][i][\"use_in_test\"]):\n",
    "\n",
    "                if (accumulator == False):\n",
    "                    X_test_total = train_test_datasets[i][3]\n",
    "                    y_test_total = train_test_datasets[i][5]\n",
    "                    accumulator = True\n",
    "                else:\n",
    "                    X_test_total = np.concatenate(\n",
    "                        (X_test_total, train_test_datasets[i][3]))\n",
    "                    y_test_total = np.concatenate(\n",
    "                        (y_test_total, train_test_datasets[i][5]))\n",
    "\n",
    "                if(len(optimized_features) > 0):\n",
    "                    y_pred = model_cv.best_estimator_.predict(\n",
    "                        train_test_datasets[i][3].iloc[:, optimized_features])\n",
    "                else:\n",
    "                    y_pred = model_cv.best_estimator_.predict(\n",
    "                        train_test_datasets[i][3])\n",
    "                crop_name = experiment[\"datasets\"][i][\"dataset_displayname\"]\n",
    "                details.append(crop_name)\n",
    "                r2Score = r2_score(train_test_datasets[i][5], y_pred)\n",
    "                rmse = np.sqrt(np.abs(mean_squared_error(train_test_datasets[i][5], y_pred)))\n",
    "                mae = mean_absolute_error(train_test_datasets[i][5], y_pred)\n",
    "                details.append(r2Score)\n",
    "                details.append(rmse)\n",
    "                details.append(mae)\n",
    "                f.write(\n",
    "                    f\"Test score for dataset {crop_name} (r2): {r2Score}\\n\")\n",
    "                f.write(\n",
    "                    f\"RMSE for dataset {crop_name}: {rmse}\\n\")\n",
    "                f.write(\n",
    "                    f\"MAE for dataset {crop_name}: {mae}\\n\")\n",
    "\n",
    "                try:\n",
    "                    msle = mean_squared_log_error(train_test_datasets[i][5], y_pred)\n",
    "                    details.append(msle)\n",
    "                    f.write(\n",
    "                        f\"MSLE for dataset {crop_name}: {msle}\\n\")\n",
    "                except Exception as e:\n",
    "                    details.append(\"N/A\")\n",
    "                    f.write(\n",
    "                        f\"MSLE is not available for {crop_name}: {e}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                final_results.append(details)\n",
    "\n",
    "        if (len(experiment[\"datasets\"]) > 1):\n",
    "            details = []\n",
    "            if(len(optimized_features) > 0):\n",
    "                y_pred_total = model_cv.best_estimator_.predict(X_test_total[:, optimized_features])\n",
    "            else:\n",
    "                y_pred_total = model_cv.best_estimator_.predict(X_test_total)\n",
    "            \n",
    "            r2Score_total = r2_score(y_test_total, y_pred_total)\n",
    "            rmse_total = np.sqrt(np.abs(mean_squared_error(y_test_total, y_pred_total)))\n",
    "            mae_total = mean_absolute_error(y_test_total, y_pred_total)\n",
    "            \n",
    "            details.append(\"Total scores\")\n",
    "            details.append(r2Score_total)\n",
    "            details.append(rmse_total)\n",
    "            details.append(mae_total)\n",
    "            \n",
    "            f.write(\n",
    "                f\"Total test score (r2): {r2Score_total}\\n\")\n",
    "            f.write(\n",
    "                f\"Total RMSE: {rmse_total}\\n\")\n",
    "            f.write(\n",
    "                f\"Total MAE: {mae_total}\\n\")\n",
    "\n",
    "            try:\n",
    "                msle_total = mean_squared_log_error(y_test_total, y_pred_total)\n",
    "                details.append(msle_total)\n",
    "                f.write(\n",
    "                    f\"Total MSLE: {msle_total}\\n\")\n",
    "            except Exception as e:\n",
    "                details.append(\"N/A\")\n",
    "                f.write(\n",
    "                    f\"Total MSLE is not available: {e}\\n\")\n",
    "                \n",
    "            final_results.append(details)\n",
    "\n",
    "        f.write(\"\\n\\n\\n\\n\")\n",
    "        f.close()\n",
    "\n",
    "    cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def load_or_train_model(file_name: str, experiment_name: str, X_train, y_train, feature_names = []):\n",
    "    optimized_indices = None\n",
    "    file_path = f\"all results/{experiment_name}/pickles/{file_name}\"\n",
    "    if (Path.exists(Path(file_path))):\n",
    "        loaded_model = pickle.load(open(file_path, 'rb'))\n",
    "        return loaded_model\n",
    "    elif (file_name.startswith(\"KNN\")):\n",
    "        model_cv = knn.train_model(X_train, y_train)\n",
    "    elif (file_name.startswith(\"Lasso\")):\n",
    "        model_cv = lasso.train_model(X_train, y_train)\n",
    "    elif (file_name.startswith(\"MLR\")):\n",
    "        model_cv = mlr.train_model(X_train, y_train)\n",
    "    elif (file_name.startswith(\"NNRegressor\")):\n",
    "        model_cv = nnreg.train_model(X_train, y_train)\n",
    "    elif (file_name.startswith(\"RF\")):\n",
    "        model_cv = rf.train_model(X_train, y_train)\n",
    "    elif (file_name.startswith(\"LR-MLP\")):\n",
    "        model_cv, optimized_indices = mlrmlp.train_model(X_train, y_train, feature_names)\n",
    "\n",
    "    # save the model to disk\n",
    "    if(optimized_indices is None):\n",
    "        pickle.dump(model_cv, open(file_path, 'wb'))\n",
    "        return model_cv\n",
    "    else:\n",
    "        pickle.dump((model_cv, optimized_indices), open(file_path, 'wb'))\n",
    "        return (model_cv, optimized_indices)\n",
    "\n",
    "\n",
    "def create_and_run_all_models(X_train: np.ndarray, y_train: np.ndarray, train_test_datasets: list, augmented: bool, experiment: dict, experiment_name: str):\n",
    "\n",
    "    knn_name = \"KNN_aug\" if augmented else \"KNN\"\n",
    "    lasso_name = \"Lasso_aug\" if augmented else \"Lasso\"\n",
    "    mlr_name = \"MLR_aug\" if augmented else \"MLR\"\n",
    "    nnreg_name = \"NNRegressor_aug\" if augmented else \"NNRegressor\"\n",
    "    rf_name = \"RF_aug\" if augmented else \"RF\"\n",
    "    lr_mlp_name = \"LR-MLP_aug\" if augmented else \"LR-MLP\"\n",
    "    \n",
    "    independent_variables = constants.independant_variables_potato if experiment[\"experiment_type\"] == \"potato\" else constants.independant_variables_other_crops\n",
    "    all_independent_feature_names = train_test_datasets[0][0].columns[independent_variables]\n",
    "    final_results.clear()\n",
    "\n",
    "    # KNN\n",
    "    print(\"Evaluating KNN model...\")\n",
    "    knn_model = load_or_train_model(f\"{knn_name}.sav\", experiment_name, X_train, y_train)\n",
    "    knn_cv_results = create_test_results(\n",
    "        knn_name, knn_model, train_test_datasets, experiment, experiment_name)\n",
    "\n",
    "    # Lasso\n",
    "    print(\"Evaluating Lasso model...\")\n",
    "    lasso_model = load_or_train_model(f\"{lasso_name}.sav\", experiment_name, X_train, y_train)\n",
    "    lasso_cv_results = create_test_results(\n",
    "        lasso_name, lasso_model, train_test_datasets, experiment, experiment_name)\n",
    "\n",
    "    # MLR\n",
    "    print(\"Evaluating MLR model...\")\n",
    "    mlr_model = load_or_train_model(f\"{mlr_name}.sav\", experiment_name, X_train, y_train)\n",
    "    mlr_cv_results = create_test_results(\n",
    "        mlr_name, mlr_model, train_test_datasets, experiment, experiment_name)\n",
    "\n",
    "    # NNRegressor\n",
    "    print(\"Evaluating NNRegressor model...\")\n",
    "    nnreg_model = load_or_train_model(f\"{nnreg_name}.sav\", experiment_name, X_train, y_train)\n",
    "    nnreg_cv_results = create_test_results(\n",
    "        nnreg_name, nnreg_model, train_test_datasets, experiment, experiment_name)\n",
    "\n",
    "    # RF\n",
    "    print(\"Evaluating RF model...\")\n",
    "    rf_model = load_or_train_model(f\"{rf_name}.sav\", experiment_name, X_train, y_train)\n",
    "    rf_cv_results = create_test_results(rf_name, rf_model, train_test_datasets, experiment, experiment_name)\n",
    "\n",
    "    # LR-MLP\n",
    "    print(\"Evaluating LR-MLP model...\")\n",
    "    lr_mlp_model, optimized_lr_mlp_features = load_or_train_model(f\"{lr_mlp_name}.sav\", experiment_name, X_train, y_train,\n",
    "                                                                  all_independent_feature_names)\n",
    "    lr_mlp_cv_results = create_test_results(\n",
    "        lr_mlp_name, lr_mlp_model, train_test_datasets, experiment, experiment_name, optimized_lr_mlp_features)\n",
    "    \n",
    "    # Create plots\n",
    "    knn.create_plot(knn_cv_results, knn_model, knn_name)\n",
    "    \n",
    "    lasso.create_plot(lasso_cv_results, lasso_model, lasso_name)\n",
    "    lasso.plot_feature_importance(lasso_model.best_estimator_.coef_,\n",
    "                                  all_independent_feature_names, lasso_name)\n",
    "    \n",
    "    mlr.create_plot(mlr_cv_results, mlr_model, mlr_name)\n",
    "    mlr.plot_feature_importance(mlr_model.best_estimator_.estimator_.coef_[\n",
    "                                0], all_independent_feature_names[mlr_model.best_estimator_.support_], mlr_name)\n",
    "    \n",
    "    rf.create_plot(rf_cv_results, rf_model, rf_name)\n",
    "    rf.plot_feature_importance(rf_model.best_estimator_.feature_importances_, all_independent_feature_names,\n",
    "                               rf_name)\n",
    "\n",
    "    write_results_to_csv(experiment_name)\n",
    "\n",
    "\n",
    "def run_experiments(): \n",
    "    for experiment in constants.experiments: # this specific function is said; for every experiment in constants.py run some models\n",
    "        experiment_name = experiment[\"title\"] #first grab title\n",
    "        print(f\"\\n\\n\\nRun model for experiment {experiment_name}\") #run models\n",
    "        #experiment type: potato or other we have four state: manual_normal , manual_aug, cft_normal, cft_aug\n",
    "        \n",
    "        #this function gives us 3output(train_test_datasets, X_train, y_train) #read_datasets_and_prepare_model will be called\n",
    "        train_test_datasets, X_train, y_train = read_datasets_and_prepare_model(\n",
    "            constants.train_test_datasets_file_name, experiment[\"datasets\"], experiment_name, False, experiment[\"experiment_type\"])\n",
    "        \n",
    "        #input parameters are dataset file_name,,datasets,experiment_name,experiment_type(from constants file)\n",
    "        create_and_run_all_models(X_train, y_train, train_test_datasets, False, experiment, experiment_name)\n",
    "\n",
    "        if (len(experiment[\"aug_datasets\"]) > 0):\n",
    "            print(f\"\\n\\n\\nRun model for experiment {experiment_name} - augmented\")\n",
    "            train_test_datasets_aug, X_train_aug, y_train_aug = read_datasets_and_prepare_model(\n",
    "                constants.train_test_datasets_aug_file_name, experiment[\"aug_datasets\"], experiment_name, False, experiment[\"experiment_type\"])\n",
    "\n",
    "            X_train = np.concatenate((X_train, X_train_aug))\n",
    "            y_train = np.concatenate((y_train, y_train_aug))\n",
    "\n",
    "            create_and_run_all_models(X_train, y_train, train_test_datasets, True, experiment, experiment_name)\n",
    "\n",
    "        if(experiment[\"generate_cft_result\"] == True):\n",
    "            experiment_name = experiment[\"title\"] + \" - cool farm tool\"\n",
    "            print(f\"\\n\\n\\nRun model for experiment {experiment_name}\")\n",
    "            \n",
    "            train_test_datasets, X_train, y_train = read_datasets_and_prepare_model(\n",
    "                constants.train_test_datasets_file_name, experiment[\"datasets\"], experiment_name, True, experiment[\"experiment_type\"])\n",
    "\n",
    "            create_and_run_all_models(X_train, y_train, train_test_datasets, False, experiment, experiment_name)\n",
    "\n",
    "            if (len(experiment[\"aug_datasets\"]) > 0):\n",
    "                print(f\"\\n\\n\\nRun model for experiment {experiment_name} - augmented\")\n",
    "                train_test_datasets_aug, X_train_aug, y_train_aug = read_datasets_and_prepare_model(\n",
    "                    constants.train_test_datasets_aug_file_name, experiment[\"aug_datasets\"], experiment_name, True, experiment[\"experiment_type\"])\n",
    "\n",
    "                X_train = np.concatenate((X_train, X_train_aug))\n",
    "                y_train = np.concatenate((y_train, y_train_aug))\n",
    "\n",
    "                create_and_run_all_models(X_train, y_train, train_test_datasets, True, experiment, experiment_name)\n",
    "\n",
    "\n",
    "initialize() #this function is for setting the parameters, creating the required folders. #in the flow of the code initialize()\n",
    "             #will be called first, and initialize will call create_output_directory,\n",
    "\n",
    "run_experiments(); #for every experiment that defined in the constants.py do some work,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
